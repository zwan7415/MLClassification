#import libraries
import pandas as pd
import numpy as np

#import plot libraries
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

#import standardlization, onehotcoder
from sklearn.preprocessing import StandardScaler 
from sklearn.preprocessing import OneHotEncoder 

from sklearn.model_selection import train_test_split

#import modeles and metrics
from sklearn import tree
from dtreeviz.trees import *
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn import metrics

import warnings
warnings.filterwarnings("ignore")


import datetime as dt
from datetime import datetime, timedelta

#loading datasets
clients = pd.read_csv('clients.csv')
customer = pd.read_csv('customers.csv')
messages = pd.read_csv('messages.csv')

**File 1: clients dataset**

#file 1 inspection
print(clients.columns.is_unique)
print(clients.shape)
print(clients[clients.duplicated()== True])
clients.info()

#change column name,prepare for datasets join
clients = clients.rename(
    columns = {'id':'client_id'})
clients.describe()

Key insight:    
--no missing values in the dataset
--all object features
--all columns are unique, no duplicate rows identified

**File 2 : customer dataset**

#File 2 inspection
print(customer.columns.is_unique)
print(customer.shape)
print(customer[customer.duplicated()==True])
customer.info()

customer.describe()

Key insight:    
-- missing values in the 'gender','country','age'
--all object features but 'age'
--all columns are unique, no duplicate rows identified

#join file 1&2 on key 'client_id'
client_customer = pd.merge(customer,clients,left_on=['client_id'], right_on=['client_id'], how = 'left')

#change column name,prepare for datasets join
client_customer = client_customer.rename(
    columns = {'country_x':'country_customer','country_y':'country_client','name':'client_name','id':'customer_id'})

**File 3: messages dataset**

# File 3 inspection
print(messages.columns.is_unique)
print(messages.shape)
print(messages[messages.duplicated()==True])
messages.info()

messages.describe()

Key insight:    
-- no missing values in the dataset
--all object features but 'clicked','converted'
-- 'clicked' and 'converted' values between 1 and 0
-- 'converted' mean lower than 'clicked'
--all columns are unique, no duplicate rows identified

#final join table with messages dataset, on key customer_id
df = pd.merge(messages,client_customer,left_on=['customer_id'], right_on=['customer_id'], how = 'left')

print(df.columns.is_unique)
print(df.shape)
print(df[df.duplicated()==True])
df.info()

df.describe()

Key insight:    
-- missing values in the dataset, 'gender', 'country_customer', 'age'
-- all object features but 'clicked','converted','age'
-- 'clicked' and 'converted' values between 1 and 0
-- 'converted' mean lower than 'clicked'
--all columns are unique, no duplicate rows identified
-- 'clicked','converted','age columns have diffrent scale range

#rename columns and drop unwanted columns 
df = df.rename(
    columns = {'sent_at':'email_sent_at','clicked':'service_clicked'})

**Create a datasets with day off hours,and weekend date**

#prepare a extra datasets about out work hours and weekend from 1/4/2021

df_dayoff = pd.date_range(start='1/4/2021',periods=6481,freq='H').to_frame()
df_dayoff = df_dayoff.rename(
    columns = {0:'email_sent_at'})
df_dayoff = df_dayoff.reset_index(drop=True)

#prepare list
gender ={'F','M'}
country ={'CA','NZ','UK'}
product_type ={'BNPL','CreditCard','Energy'}

df_dayoff['service_clicked']= np.random.choice((0,1),size=df_dayoff.shape[0]).astype(float)
df_dayoff['converted']= 0
df_dayoff['gender']= np.random.choice(list(gender),size=df_dayoff.shape[0])
df_dayoff['country_customer']= np.random.choice(list(country),size=df_dayoff.shape[0])
df_dayoff['age']= np.random.randint(18,90, size=df_dayoff.shape[0]).astype(float)
df_dayoff['product_type']= np.random.choice(list(product_type),size=df_dayoff.shape[0])

#one hot coding, add features
encoderoff = OneHotEncoder(sparse =False)
encodeoff_cols= ['gender', 'product_type','country_customer']

df_encodedoff = pd.DataFrame(encoderoff.fit_transform(df_dayoff[encodeoff_cols]))
df_encodedoff.columns = encoderoff.get_feature_names(encodeoff_cols)

# Replace Categotical Data with Encoded Data
df_dayoff = df_dayoff.drop(encodeoff_cols,axis = 1)
df_dayoff = df_dayoff.reset_index(drop=True)
df_dayoff = pd.concat([df_encodedoff, df_dayoff], axis = 1)

#set index as date column
df_dayoff= df_dayoff.set_index('email_sent_at')
df_dayoff.index=pd.to_datetime(df_dayoff.index)

#add time series features based on time series index.
def creat_features(df_dayoff):
    df_dayoff['hour']=df_dayoff.index.hour
    df_dayoff['dayofweek']=df_dayoff.index.day_of_week
    df_dayoff['dayofyear']=df_dayoff.index.dayofyear
    df_dayoff['isweekend'] = np.where((df_dayoff['dayofweek'] == 5) | (df_dayoff['dayofweek'] == 6), 1, 0)
    return df_dayoff

df_dayoff = creat_features(df_dayoff)

#creat weekend dataset, set 'converted' is zero 
df_weekend = df_dayoff[df_dayoff['isweekend'].isin([1])]

#prepare list
offhr_ca ={0,1,2,3,4,5,6,7,8,17,18,19,20,21,22,23}
offhr_nz ={0,1,2,3,4,5,6,7,8,18,19,20,21,22,23}
offhr_uk ={0,1,2,3,4,5,6,7,20,21,22,23}

#creat CA weekday offhour datasets, set 'converted' is zero 
df_week = df_dayoff[df_dayoff['isweekend'].isin([0])]
df_weekca = df_week[df_week['country_customer_CA'].isin([1])]
df_weekca = df_weekca[df_weekca['hour'].isin(list(offhr_ca))]

#creat NZ weekday offhour datasets, set 'converted' is zero 
df_weeknz = df_week[df_week['country_customer_NZ'].isin([1])]
df_weeknz = df_weeknz[df_weeknz['hour'].isin(list(offhr_nz))]

#creat UK weekday offhour datasets, set 'converted' is zero 
df_weekuk = df_week[df_week['country_customer_UK'].isin([1])]
df_weekuk = df_weekuk[df_weekuk['hour'].isin(list(offhr_uk))]

#merging all weekend and offhours datasets in each country
df_off = pd.concat([df_weekend,df_weekca,df_weeknz,df_weekuk])

#drop not use column
df_off = df_off.drop(['isweekend'],axis = 1)
df_off.head()

df_off.reset_index(inplace=True)

**Prepare orignol datasets**

df=df.drop(columns=['id', 'customer_id','client_id','created_at'])

#fix missing values
#fill NaN in 'country_customer' column with 'country_client' value
df['country_customer']=df['country_customer'].fillna(df['country_client'])

#fill missing values
#fill NaN in 'age' column with median values
#groupby remove rows where gender is NaN 
df = df.groupby(['gender'], group_keys=False).apply(lambda x: x.fillna(x.median()))

df.info()

Key insight:   

no missing values in the dataset
all object features but 'service_clicked','converted','age'
'clicked' and 'converted' values between 1 and 0
'converted' mean lower than 'clicked'
all columns are unique, no duplicate rows identified

'clicked','converted','age columns have diffrent scale range

'age','gender','service_clicked' and ' product type' have strong to medium association with target,analysised from EDA section

#feature creation: one hot coding, add features
encoder = OneHotEncoder(sparse =False)
encode_cols= ['gender', 'product_type','country_customer']

df_encoded = pd.DataFrame(encoder.fit_transform(df[encode_cols]))
df_encoded.columns = encoder.get_feature_names(encode_cols)

# Replace Categotical Data with Encoded Data
df = df.drop(encode_cols,axis = 1)
df = df.reset_index(drop=True)
df = pd.concat([df_encoded, df], axis = 1)

print('Shape of dataframe:', df.shape)

#drop none use featuers
df = df.drop(['client_name','country_client'],axis = 1)
df = df.reset_index(drop=True)

df['email_sent_at'] = pd.to_datetime(df['email_sent_at'])
type(df['email_sent_at'])

#set email set at as index , prepare add feature, split train set and test set
df= df.set_index('email_sent_at')
df.index=pd.to_datetime(df.index)
df.head()

#count target values
df['converted'].value_counts()

**add more features**

feature creation: creat time series features based on time series index.

add: hour, day of week and day of year features

#add more features
#feature creation: creat time series features based on time series index.

def creat_features(df):
    df['hour']=df.index.hour
    df['dayofweek']=df.index.day_of_week#.map(day_mapping)
    df['dayofyear']=df.index.dayofyear
    return df
df = creat_features(df)
df.head()

df.info()

**Standarlization/trim outliers**

#duplicate datasets
df_copy = df.copy()

#standarlization for trim outliers
#scaler = StandardScaler()
#num_cols = ['age','dayofyear']
#df_copy[num_cols]=scaler.fit_transform(df[num_cols])

#there are two integer columns diffrent orther colunms

df_copy.head()

df_copy.reset_index(inplace=True)
df_copy.head()

#change series data type to object 
df_copy['email_sent_at'].dt
df_copy.head()

**merging original and offhours datasets,increase size of datasets**

#merging all weekend and offhours datasets in each country
df_copy = pd.concat([df_copy,df_off])
df_copy.head()

df_copy.info()

df_copy= df_copy.set_index('email_sent_at')
df_copy.index=pd.to_datetime(df_copy.index,utc=True)

df_copy.reset_index(inplace=True)

df_copy.info()

#Sort the email_sent_at column by time
df_copy= df_copy.sort_values('email_sent_at')

df_copy= df_copy.set_index('email_sent_at')
df_copy.index=pd.to_datetime(df_copy.index,utc=True)

#reset index
df_copy.reset_index(inplace=True)

#find row#/date at 60%,50%,70%,80% split points,for training model later

print(df_copy.loc[[83199]])#7:3
#print(df_copy.loc[[95085]])#8:2
print(df_copy.loc[[59429]])#5:5
print(df_copy.loc[[71314]])#6:4

#set date time as index for the final dataset
df_copy= df_copy.set_index('email_sent_at')
df_copy.index=pd.to_datetime(df_copy.index,utc=True)

**Creat predication model**

#  split dataset with ratio 70:30 
#70% at 2021-08-13

test_date = '2021-08-13'

target = 'converted'
features= [col for col in df_copy.columns if col != target]

train_data = df_copy.loc[df_copy.index <= test_date]
test_data = df_copy.loc[df_copy.index > test_date]


#X_train, X_test, y_train, y_test = train_test_split(feature , target, shuffle = True, test_size=0.3)

x_train = train_data[features]
y_train = train_data[target]
x_test = test_data[features]
y_test = test_data[target]

# Show the Training and Testing Data
print('Shape of training feature:', x_train.shape)
print('Shape of testing feature:', x_test.shape)
print('Shape of training label:', y_train.shape)
print('Shape of testing label:', y_test.shape)

**Evaluate model with metrics for classification problem**

#Evaluate model with metrics: accuracy, precision, recall, F1 score, confussion matrix and AUC

def evaluate_model(model, x_test, y_test):
    
    # Predict Test Data 
    y_pred = model.predict(x_test)

    # Calculate accuracy, precision, recall, f1-score,
    acc = metrics.accuracy_score(y_test, y_pred)
    prec = metrics.precision_score(y_test, y_pred)
    rec = metrics.recall_score(y_test, y_pred)
    f1 = metrics.f1_score(y_test, y_pred)

    # Calculate area under curve (AUC)
    y_pred_proba = model.predict_proba(x_test)[::,1]
    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)
    auc = metrics.roc_auc_score(y_test, y_pred_proba)

    # Display confussion matrix
    cm = metrics.confusion_matrix(y_test, y_pred)

    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 
            'fpr': fpr, 'tpr': tpr, 'auc': auc, 'cm': cm}

**Model comparison**

Model1 : Random forest

Model2 : K-nearest neighbors

Model3 : Decision tree

Model4 : use RandomSearchCV find the best model

#Model 1: Random forest
random_forest = RandomForestClassifier(random_state=0)
random_forest.fit(x_train,y_train)

# Evaluate Model 1 
rf_eval = evaluate_model(random_forest, x_test, y_test)

# Print result
print('Accuracy:', rf_eval['acc'])
print('Precision:', rf_eval['prec'])
print('Recall:', rf_eval['rec'])
print('F1 Score:', rf_eval['f1'])
print('Area Under Curve:', rf_eval['auc'])
print('Confusion Matrix:\n', rf_eval['cm'])

#Model 2 : K-nearest Neighbors
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)

# Evaluate Model 2 
knn_eval = evaluate_model(knn, x_test, y_test)

# Print result
print('Accuracy:', knn_eval['acc'])
print('Precision:', knn_eval['prec'])
print('Recall:', knn_eval['rec'])
print('F1 Score:', knn_eval['f1'])
print('Area Under Curve:', knn_eval['auc'])
print('Confusion Matrix:\n', knn_eval['cm'])

#Model 3 : Decision tree
decision_tree = tree.DecisionTreeClassifier(random_state=0,class_weight='balanced')
decision_tree.fit(x_train, y_train)

#evaluate model 3:
dtree_eval = evaluate_model(decision_tree,x_test,y_test)

print('Accuracy: ', dtree_eval['acc'])
print('Precision: ', dtree_eval['prec'])
print('Recall: ', dtree_eval['rec'])
print('F1 Score: ', dtree_eval['f1'])
print('Area under Curve: ', dtree_eval['auc'])
print('Confusion Matrix: ', dtree_eval['cm'])

# Comparing ROC Curve
sns.set_theme(style='whitegrid')
fig,ax = plt.subplots(figsize=(6,5))
fig,ax.plot(dtree_eval['fpr'], dtree_eval['tpr'], label='Decision Tree, auc = {:0.5f}'.format(dtree_eval['auc']))
fig,ax.plot(rf_eval['fpr'], rf_eval['tpr'], label='Random Forest, auc = {:0.5f}'.format(rf_eval['auc']))
fig,ax.plot(knn_eval['fpr'], knn_eval['tpr'], label='K-Nearest Nieghbor, auc = {:0.5f}'.format(knn_eval['auc']))


# label x and y axis
ax.set_xlabel('False Positive Rate', fontweight='bold')
ax.set_ylabel('True Positive Rate', fontweight='bold')

# Create legend & title
ax.set_title('ROC Curve', fontsize=14, fontweight='bold')
ax.legend(loc=4)

random_forest.feature_importances_

df_copy.columns

# basic random forest model have hightest auc socre
# turn on parameters for basic random forest
# create the parameter dictionary based on the results of random search
#The initial value for each parameter is [10,100,1000,10000], and the value range is narrowed after the test.
# scoring choice f1, becuase the dataset is inbalanced dataet.

param_grid = {
    'max_depth': [50,100,150,200,250],
    #'max_features': [4,6,8,10],
    'min_samples_leaf': [2,3,4,5,6],
    'min_samples_split': [50,100,150,200,250],
    'n_estimators': [50,100,150,200,250]
}

# create a base model,turn on class_weight,reduce data inbalence
rf_grids = RandomForestClassifier(random_state=0,class_weight='balanced')

# initiate the grid search model,use bagging/folds reduce variance,focus on F1 score
random_search = RandomizedSearchCV(estimator=rf_grids,param_distributions = param_grid, scoring='f1',cv=5, n_jobs=-1, verbose=2)


# fit the grid search to the train sets
random_search.fit(x_train, y_train)

random_search.best_params_

# save best model (model4) with best fit
best_search = random_search.best_estimator_

#evaluate Model4
best_search_eval = evaluate_model(best_search, x_test, y_test)

#print result
print('Accuracy:', best_search_eval['acc'])
print('Precision:', best_search_eval['prec'])
print('Recall:', best_search_eval['rec'])
print('F1 Score:', best_search_eval['f1'])
print('Area Under Curve:', best_search_eval['auc'])
print('Confusion Matrix:\n', best_search_eval['cm'])

#compare evaluate result between basic random forest model and model4
print('Change of {:0.2f}% on accuracy.'.format(100 * ((best_search_eval['acc'] - rf_eval['acc']) / rf_eval['acc'])))
print('Change of {:0.2f}% on precision.'.format(100 * ((best_search_eval['prec'] - rf_eval['prec']) / rf_eval['prec'])))
print('Change of {:0.2f}% on recall.'.format(100 * ((best_search_eval['rec'] - rf_eval['rec']) / rf_eval['rec'])))
print('Change of {:0.2f}% on F1 score.'.format(100 * ((best_search_eval['f1'] - rf_eval['f1']) / rf_eval['f1'])))
print('Change of {:0.2f}% on AUC.'.format(100 * ((best_search_eval['auc'] - rf_eval['auc']) / rf_eval['auc'])))

Recall, F1 score, AUC metrics score increased , especially recall score increased 160%. Although accuracy score was slightly dropped, the best_ best_serach model succused. A system with high recall but low precision (0.32) returns, is because some predicted labels are incorrect when compared to the training labels. However, a high recall score (0.97) means that the model covered the most of the correct labels. 

#test model predication
y_pred=best_search.predict(x_test)
print(y_pred)

**File 4: sample_customer_ids dataset,and test prediction**

#loading the test dataset,double the amount of data for include dayoff hours and weekends  
sample_customer = pd.read_csv('sample_customer_ids2.csv')

#expend datasets,add columns,create dummy data
#date from 4/Oct/2021

sample_customer['email_sent_at'] = pd.date_range(start='10/4/2021', periods=len(sample_customer),freq='H')
sample_customer['service_clicked']= np.random.choice((0,1),size=sample_customer.shape[0]).astype(float)
sample_customer['age']= np.random.randint(18,90, size=sample_customer.shape[0]).astype(float)
sample_customer['converted']= np.random.choice((0,1),size=sample_customer.shape[0]).astype(float)

product_type2 ={'BNPL','CreditCard','Energy'}
sample_customer['product_type']= np.random.choice(list(product_type2),size=sample_customer.shape[0])
gender2 ={'F','M'}
sample_customer['gender']= np.random.choice(list(gender2),size=sample_customer.shape[0])
country2 ={'CA','NZ','UK'}
sample_customer['country_customer']= np.random.choice(list(country2),size=sample_customer.shape[0])

#one hot coding, add features
encoder3 = OneHotEncoder(sparse =False)
encode_cols3= ['gender', 'product_type','country_customer']

sample_encoded3 = pd.DataFrame(encoder3.fit_transform(sample_customer[encode_cols3]))
sample_encoded3.columns = encoder3.get_feature_names(encode_cols3)

# Replace Categotical Data with Encoded Data
sample_customer = sample_customer.drop(encode_cols3,axis = 1)
sample_customer = sample_customer.reset_index(drop=True)
sample_customer = pd.concat([sample_encoded3, sample_customer], axis = 1)

print('Shape of dataframe:', sample_customer.shape)

#set index as date column
sample_customer= sample_customer.set_index('email_sent_at')
sample_customer.index=pd.to_datetime(sample_customer.index)

#add time series features

def creat_timefeatures(sample_customer):
    sample_customer['hour']=sample_customer.index.hour
    sample_customer['dayofweek']=sample_customer.index.day_of_week
    sample_customer['dayofyear']=sample_customer.index.dayofyear
    return sample_customer

sample_customer = creat_timefeatures(sample_customer)

#drop not use column
sample_customer = sample_customer.drop(['customer_id'],axis = 1)
sample_customer.head()

#group prediction features
target2 = 'converted'
features2=[col for col in sample_customer.columns if col != target]
x_test2 = sample_customer[features2]

#run model for new datasets, test run
sample_customer['converted_prediction'] = best_search.predict(x_test2)
sample_customer['converted_prediction'] = sample_customer['converted_prediction'].apply(lambda x: 'yes' if x==1 else 'no')
sample_customer.head()

#ralte to hour?
# plot and count number of customer in hour

hr_count= sns.catplot(x='hour', kind='count',hue = 'converted_prediction',palette='Spectral', data=sample_customer)

# add label and title
hr_count.set(xlabel = 'Hour/Day', ylabel = '', title = 'Number of Service Converted in Each Hour')

#ralte to day of week?
# plot and count number of customer in day of week
# 0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'

week_count= sns.catplot(x='dayofweek', kind='count',hue = 'converted_prediction',palette='Spectral', data=sample_customer)

# add label and title
week_count.set(xlabel = 'Day of Week', ylabel = '', title = 'Number of Service Converted in Day of Week')

plt.show()

In the test data set, there is no converted_prediction marked "yes" after 20:00 and before 8am.

no converted_predictions marked "yes" over the weekend.

The optimized model predicts zero conversion for customers during dayoff hours and weekends, therefore customers will not receive emails from the company during dayoff hours and weekends.
